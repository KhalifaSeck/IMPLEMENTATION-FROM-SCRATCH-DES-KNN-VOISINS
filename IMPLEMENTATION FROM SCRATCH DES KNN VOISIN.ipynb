{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`IMPLEMENTATION FROM SCRATCH DES K VOISINS LES PLUS PROCHES`**\n",
    "\n",
    "L'algorithme **KNN (K-nearest neighbors)** est un modèle de Machine Learning supervisé, c'est-à-dire qui se base sur des données d'entrée étiquetées.<br> \n",
    "C'est en apprenant de ce type de données qu'il sera en mesure de proposer une sortie appropriée en partant maintenant de nouvelles données non étiquetées. L'algorithme peut être appliqué avec le langage de programmation Python à partir de fichier csv. KNN des plus proches voisins s'utilise pour la **régression** et la **classification** des données.\n",
    "\n",
    "Dans la pratique, KNN n'aura pas besoin de passer par la construction d'un modèle prédictif pour réaliser des prédictions. L'algorithme classe les nouvelles données en se basant sur le jeu de données précédant pour fournir des résultats. Son principe se décrit par cette affirmation : **« Dis-moi qui sont tes voisins, je te dirais qui tu es »**.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`RESUME DES DIFFERENTES ETAPES DE LA METHODOLOGIE CRISP-DM`**\n",
    "### **`I. DESCRIPTION ET OBJECTIF DE NOTRE ETUDE`**\n",
    "### **`II. IMPLEMENTATION ET APPLICATION DES KNN POUR LA CLASSIFICATION`**\n",
    "#### **`II.1. IMPLEMENTATION DES KNN POUR LA CLASSIFICATION`**  \n",
    "#### **`II.2. APPLICATION DES KNN POUR LA CLASSIFICATION DANS LA BASE DE DONNEES IRIS`**\n",
    "\n",
    "### **`III. IMPLEMENTATION DES KNN POUR LA REGRESSION`**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`I. DESCRIPTION ET OBJECTIF DE NOTRE ETUDE`**\n",
    "\n",
    "\n",
    "**-** K-plus proches voisins (KNN) est un type d'algorithme d'apprentissage supervisé utilisé à la fois pour la régression et la classification.<br> \n",
    "KNN essaie de prédire la classe correcte pour les données de test en calculant la distance entre les données de test et tous les points d'apprentissage<br> Sélectionnez ensuite le nombre K de points qui est le plus proche des données de test.<br> \n",
    "\n",
    "L'algorithme KNN calcule la probabilité que les données de test appartiennent aux classes de données d'apprentissage 'K' et que la classe détient la probabilité la plus élevée sera sélectionnée. Dans le cas d'une régression, la valeur est la moyenne des 'K' points d'apprentissage sélectionnés.<br>\n",
    "\n",
    "L’algorithme k-NN est une méthode non-paramétrique d’apprentissage supervisé qui peut être utilisée pour résoudre des problèmes de régression et de classification. Il est basé sur la similarité entre les instances de données et est intuitif et facile à comprendre.\n",
    "\n",
    "\n",
    "\n",
    "**-** C'est dans cette perspective que nous nous fixons comme objectif d'implémenter from scratch l’algorithme des k voisins les plus proches (k-NN) pour la régression et la classification en utilisant la programmation orientée objet et en tenant compte des hyperparamètres tels que la distance (**euclidienne ou manhattan**),le nombre de voisins(k), la pondération des votes(**distance ou uniforme**), la normalisation des caractéristiques et la méthode d'indexation spatiale(**k-d tree ou ball tree**) .<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`II. IMPLEMENTATION ET APPLICATION DES (KNN) POUR LA CLASSIFICATION`**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`II.1. IMPLEMENTATION DES KNN POUR LA CLASSIFICATION`**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des librairies utiles\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter                     # Pou le compte des k\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler   # Pour la normalisation des caractéristiques\n",
    "from sklearn.neighbors import KDTree, BallTree     # Pour la méthode d'indexation\n",
    "\n",
    "import warnings                                    # Pour éviter la génération des warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnnClassification:\n",
    "    # Constructeur\n",
    "    \n",
    "    def __init__(self, distance ='euclidienne',k=3, ponderation='distance', methode_indexation='kd_tree'):\n",
    "\n",
    "        self.k = k\n",
    "        self.distance = distance\n",
    "        self.ponderation = ponderation\n",
    "        self.methode_indexation = methode_indexation\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.tree = None\n",
    "\n",
    "    # Fonction d'entrainement\n",
    "\n",
    "    def entrainement(self, X_train, y_train):\n",
    "    \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        if self.methode_indexation == 'kd_tree':\n",
    "            self.tree = KDTree(self.X_train)\n",
    "        elif self.methode_indexation == 'ball_tree':\n",
    "            self.tree = BallTree(self.X_train)\n",
    "        else:\n",
    "            raise ValueError(\"Vos méthodes d'indexation sont invalides, il ne faut choisir qu'une parmi : KDTree et BallTree\")\n",
    "        \n",
    "\n",
    "    # Fonction de calcul de la distance\n",
    "\n",
    "    def mesure_distance(self, Xi, Yi):\n",
    "      \n",
    "        # Distance euclidienne : plus la distance est petite, plus les deux observations sont identiques.\n",
    "        if ( self.distance == 'euclidienne'):\n",
    "            distanc = np.sqrt(np.sum((Xi - Yi) ** 2))\n",
    "            \n",
    "            return distanc\n",
    "            \n",
    "        # Distance manhattan : connue sous le nom de distance en paté de maisons, elle est la somme des differences absolues de leurs coordonnées cartesiennes.\n",
    "        elif (self.distance == 'manhattan'):\n",
    "            distanc = np.sum(np.abs(Xi - Yi))\n",
    "\n",
    "            return distanc\n",
    "        \n",
    "\n",
    "    # Fonction de prédiction\n",
    "\n",
    "    def predictClassif(self, X_test):\n",
    "        y_pred = []\n",
    "\n",
    "        for point_test in X_test:\n",
    "            distances = []\n",
    "\n",
    "            # Calcul des distances entre l'échantillon de test et tous les échantillons d'entraînement\n",
    "            for point_train in self.X_train:\n",
    "                d = self.mesure_distance(point_test, point_train)\n",
    "                distances.append(d)\n",
    "\n",
    "            # Tri des distances et récupération des indices des k plus proches voisins\n",
    "            indices = np.argsort(distances)[:self.k]\n",
    "            k_indices_proches = np.array(distances)[indices]\n",
    "\n",
    "            # Calcul des poids inverses des distances\n",
    "            poids = 1.0 / k_indices_proches\n",
    "\n",
    "            # Calcul des poids normalisés\n",
    "            poids = poids / np.sum(poids)\n",
    "\n",
    "            # Prédiction de l'étiquette en utilisant les poids pondérés\n",
    "            k_proches = self.y_train[indices]\n",
    "            k_voisins_ponderes = np.dot(poids, k_proches).flatten()   # Convertir les k_voisins_ponderes en un tableau\n",
    "            k_voisins_ponderes = np.round(k_voisins_ponderes).astype(int)\n",
    "\n",
    "            # Supprimer les valeurs négatives de k_voisins_ponderes\n",
    "            k_voisins_ponderes = [x for x in k_voisins_ponderes if x >= 0]\n",
    "\n",
    "            # Calculer le mode en utilisant np.bincount()\n",
    "            if len(k_voisins_ponderes) > 0:\n",
    "                mode_value = np.argmax(np.bincount(k_voisins_ponderes))\n",
    "           \n",
    "            y_pred.append(mode_value)\n",
    "\n",
    "            predictions = np.array(y_pred)\n",
    "           \n",
    "        return predictions\n",
    "    \n",
    "\n",
    "    # Fonction de calcul de l'accuracy \n",
    "\n",
    "    def accuracy_knn(self, y_vraie, y_pred): \n",
    "        correctes = 0\n",
    "        for i in range(len(y_vraie)): \n",
    "            if y_vraie[i] == y_pred[i]: \n",
    "                correctes = correctes + 1\n",
    "        return (correctes / float(len(y_vraie)))*100\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`II.1. APPLICATION DES KNN POUR LA CLASSIFICATION DANS LA BASE DE DONNEES IRIS`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation du jeu de données\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération du dataset, et partitionnement de ce dernier\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions X_train : (120, 4)\n",
      "Dimensions y_train : (120,)\n",
      "Dimensions X_test : (30, 4)\n",
      "Dimensions y_test : (30,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions X_train :\", X_train.shape)\n",
    "print(\"Dimensions y_train :\", y_train.shape)\n",
    "print(\"Dimensions X_test :\", X_test.shape)\n",
    "print(\"Dimensions y_test :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation du dataset (ou les caratéristiques de la base)\n",
    "scaler = StandardScaler().fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation de notre algorithme\n",
    "knn = KnnClassification(k=3,distance='euclidienne', ponderation='distance', methode_indexation='kd_tree')\n",
    "# Entrainement du modèle\n",
    "knn.entrainement(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(120,)\n"
     ]
    }
   ],
   "source": [
    "print(knn.X_train.shape)\n",
    "print(knn.y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prédiction de notre algoriyhme de classification\n",
    "predictions = knn.predictClassif(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance de notre modèle \n",
    "knn.accuracy_knn(y_test,predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`II. IMPLEMENTATION DES KNN POUR LA REGRESSION`**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous ne calculons plus le mode pour les étiquettes des voisins pondérés. Au lieu de cela, nous utilisons la moyenne pondérée des valeurs cibles des k plus proches voisins pour prédire la valeur continue pour chaque point de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnnRegression:\n",
    "    # Constructeur\n",
    "    \n",
    "    def __init__(self, distance ='euclidienne',k=3, ponderation='distance', methode_indexation='kd_tree'):\n",
    "\n",
    "        self.k = k\n",
    "        self.distance = distance\n",
    "        self.ponderation = ponderation\n",
    "        self.methode_indexation = methode_indexation\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.tree = None\n",
    "\n",
    "    # Fonction d'entrainement\n",
    "\n",
    "    def entrainement(self, X_train, y_train):\n",
    "    \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        if self.methode_indexation == 'kd_tree':\n",
    "            self.tree = KDTree(self.X_train)\n",
    "        elif self.methode_indexation == 'ball_tree':\n",
    "            self.tree = BallTree(self.X_train)\n",
    "        else:\n",
    "            raise ValueError(\"Vos méthodes d'indexation sont invalides, il ne faut choisir qu'une parmi : KDTree et BallTree\")\n",
    "        \n",
    "\n",
    "    # Fonction de calcul de la distance\n",
    "\n",
    "    def mesure_distance(self, Xi, Yi):\n",
    "      \n",
    "        # Distance euclidienne : plus la distance est petite, plus les deux observations sont identiques.\n",
    "        if ( self.distance == 'euclidienne'):\n",
    "            distanc = np.sqrt(np.sum((Xi - Yi) ** 2))\n",
    "            \n",
    "            return distanc\n",
    "            \n",
    "                    # Distance manhattan : connue sous le nom de distance en paté de maisons, elle est la somme des differences absolues de leurs coordonnées cartesiennes.\n",
    "        elif (self.distance == 'manhattan'):\n",
    "            distanc = np.sum(np.abs(Xi - Yi))\n",
    "\n",
    "            return distanc\n",
    "    \n",
    "    # Fonction de prédiction\n",
    "\n",
    "    def predictReg(self, X_test):\n",
    "        y_pred = []\n",
    "\n",
    "        for point_test in X_test:\n",
    "            distances = []\n",
    "\n",
    "        # Calcul des distances entre l'échantillon de test et tous les échantillons d'entraînement\n",
    "            for point_train in self.X_train:\n",
    "                d = self.mesure_distance(point_test, point_train)\n",
    "                distances.append(d)\n",
    "\n",
    "            # Tri des distances et récupération des indices des k plus proches voisins\n",
    "            indices = np.argsort(distances)[:self.k]\n",
    "            k_indices_proches = np.array(distances)[indices]\n",
    "\n",
    "            # Calcul des poids inverses des distances\n",
    "            poids = 1.0 / k_indices_proches\n",
    "\n",
    "            # Calcul des poids normalisés\n",
    "            poids = poids / np.sum(poids)\n",
    "\n",
    "            # Prédiction de l'étiquette en utilisant les poids pondérés\n",
    "            k_proches = self.y_train[indices]\n",
    "            k_voisins_ponderes = np.dot(poids, k_proches).flatten()   # Convertir les k_voisins_ponderes en un tableau\n",
    "            k_voisins_ponderes = np.round(k_voisins_ponderes).astype(int)\n",
    "            k_voisins_ponderes = [x for x in k_voisins_ponderes if x >= 0]\n",
    "            \n",
    "            # Calculer la moyenne en utilisant np.mean()\n",
    "            if len(k_voisins_ponderes) > 0:\n",
    "                prediction = np.mean(k_voisins_ponderes)\n",
    "            else:\n",
    "                prediction = 0.0\n",
    "\n",
    "            y_pred.append(prediction)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "\n",
    "    # Fonction de calcul du score\n",
    "\n",
    "    def score(self , y , y_pred):\n",
    "        RMSE = np.sqrt(np.mean((y - y_pred)**2))\n",
    "\n",
    "        return RMSE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
